{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb134d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras import Model, layers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, Input, Conv2D, MaxPooling2D, Flatten,MaxPooling3D\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/kaggle/input/intel-image-classification/'\n",
    "train_pred_test_folders = os.listdir(root_path)\n",
    "\n",
    "seg_train_folders = '../input/intel-image-classification/seg_train/seg_train/' #one more seg_train folder within\n",
    "seg_test_folders = '../input/intel-image-classification/seg_test/seg_test/'\n",
    "seg_pred_folders = '../input/intel-image-classification/seg_pred/seg_pred/'\n",
    "quantity_tr = {} \n",
    "quantity_te = {}\n",
    "for folder in os.listdir(seg_train_folders):\n",
    "    quantity_tr[folder] = len(os.listdir(seg_train_folders+folder))\n",
    "\n",
    "for folder in os.listdir(seg_test_folders):\n",
    "    quantity_te[folder] = len(os.listdir(seg_test_folders+folder))\n",
    "    \n",
    "quantity_train = pd.DataFrame(list(quantity_tr.items()), index=range(0,len(quantity_tr)), columns=['class','count'])\n",
    "quantity_test = pd.DataFrame(list(quantity_te.items()), index=range(0,len(quantity_te)), columns=['class','count'])\n",
    "\n",
    "figure, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.barplot(x='class',y='count',data=quantity_train,ax=ax[0])\n",
    "sns.barplot(x='class',y='count',data=quantity_test,ax=ax[1])\n",
    "\n",
    "print(\"Number of images in the train set : \", sum(quantity_tr.values()))\n",
    "print(\"Number of images in the test set ; \",sum(quantity_te.values()))\n",
    "number_of_images_in_prediction_set = len(os.listdir(seg_pred_folders))\n",
    "print(\"Number of images in prediction set : \",number_of_images_in_prediction_set)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497da658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, model_name):\n",
    "    #convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = model_name+'_history.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = model_name+'_history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "def plot_accuracy_from_history(history, isinception=False):\n",
    "    color = sns.color_palette()\n",
    "    if(isinception == False):\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "    else:\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "    \n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    sns.lineplot(epochs, acc, label='Training Accuracy')\n",
    "    sns.lineplot(epochs, val_acc,label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss_from_history(history):\n",
    "    color = sns.color_palette()\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    sns.lineplot(epochs, loss,label='Training Loss')\n",
    "    sns.lineplot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    \n",
    "def do_history_stuff(history, history_file_name, isinception=False):\n",
    "    save_history(history, history_file_name)\n",
    "    plot_accuracy_from_history(history, isinception)\n",
    "    plot_loss_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator( rescale = 1.0/255.,shear_range=0.2,zoom_range=0.2)\n",
    "\n",
    "# we are rescaling by 1.0/255 to normalize the rgb values if they are in range 0-255 the values are too high for good model performance. \n",
    "train_generator = train_datagen.flow_from_directory(seg_train_folders,\n",
    "                                                    batch_size=32,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1.0/255.) #we are only normalising to make the prediction, the other parameters were used for agumentation and train weights\n",
    "validation_generator = validation_datagen.flow_from_directory(seg_test_folders, shuffle=True, batch_size=1, class_mode='categorical', target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map_classes = {v: k for k, v in validation_generator.class_indices.items()}\n",
    "print(validation_generator.class_indices)\n",
    "print(inv_map_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe731a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_few_images(number_of_examples=2, predict_using_model=None):\n",
    "    figure1, ax1 = plt.subplots(number_of_examples,len(os.listdir(seg_train_folders)), figsize=(20,4*number_of_examples))\n",
    "    ax1 = ax1.reshape(-1)\n",
    "    axoff_fun = np.vectorize(lambda ax:ax.axis('off'))\n",
    "    axoff_fun(ax1)\n",
    "    axs = 0\n",
    "    for i, folder in enumerate(os.listdir(seg_train_folders)):\n",
    "        image_ids = os.listdir(os.path.join(seg_train_folders,folder))\n",
    "        for j in [random.randrange(0, len(image_ids)) for i in range(0,number_of_examples)]:\n",
    "            \n",
    "            display = plt.imread(os.path.join(seg_train_folders,folder,image_ids[j]))\n",
    "            plt.axis('off')\n",
    "            ax1[axs].imshow(display)\n",
    "            title = 'True:'+folder\n",
    "            if(predict_using_model):\n",
    "                predicted_classname = inv_map_classes[np.argmax(inception_best_model.predict(np.array([display])))]\n",
    "                title = title+'\\nPredict :'+predicted_classname\n",
    "            ax1[axs].set_title(title)\n",
    "            axs=axs+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50934094",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# epoch config\n",
    "benchmark_epoch = 60\n",
    "vgg_epoch = 60\n",
    "resnet_epoch = 60\n",
    "inception_epoch = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random architecture\n",
    "benchmark_model = Sequential()\n",
    "# Input here is 4D array (batchsize, height, width, channels) - we have already created the train_generator with batch size 32\n",
    "# 32 Images of size each 150x150 with 3 color channels will be input into this layer\n",
    "benchmark_model.add(Conv2D(128, kernel_size=7, activation='relu', input_shape=(150,150,3)))\n",
    "benchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\n",
    "benchmark_model.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "benchmark_model.add(MaxPooling2D(pool_size=(4,4), strides=(2,2)))\n",
    "benchmark_model.add(Flatten())\n",
    "benchmark_model.add(Dense(128,activation='relu'))\n",
    "benchmark_model.add(Dense(6,activation='softmax'))\n",
    "\n",
    "benchmark_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "benchmark_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"bench_mark_-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, min_lr=0.000002)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "history = benchmark_model.fit(train_generator,epochs=benchmark_epoch, verbose=1, validation_data = validation_generator,callbacks=[reduce_lr,early_stopping,checkpoint])\n",
    "\n",
    "benchmark_model.save(filepath)\n",
    "do_history_stuff(history, 'benchmark_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(pooling='avg', weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "for layers in vgg16_model.layers:\n",
    "            layers.trainable=False\n",
    "last_output = vgg16_model.layers[-1].output\n",
    "vgg_x = Flatten()(last_output)\n",
    "vgg_x = Dense(128, activation = 'relu')(vgg_x)\n",
    "vgg_x = Dense(6, activation = 'softmax')(vgg_x)\n",
    "vgg16_final_model = Model(vgg16_model.input, vgg_x)\n",
    "vgg16_final_model.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics=['acc'])\n",
    "\n",
    "# VGG16\n",
    "number_of_epochs = vgg_epoch\n",
    "vgg16_filepath = 'vgg_16_'+'-saved-model-{epoch:02d}-acc-{val_acc:.2f}.hdf5'\n",
    "vgg_checkpoint = tf.keras.callbacks.ModelCheckpoint(vgg16_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "vgg_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "vgg16_history = vgg16_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=[vgg_checkpoint,vgg_early_stopping],verbose=1)\n",
    "\n",
    "do_history_stuff(vgg16_history, 'vgg16_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750374ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150,150,3), classes=6)\n",
    "\n",
    "for layers in ResNet50_model.layers:\n",
    "    layers.trainable=True\n",
    "\n",
    "opt = SGD(lr=0.01,momentum=0.7)\n",
    "# resnet50_x = Conv2D(64, (3, 3), activation='relu')(ResNet50_model.output)\n",
    "# resnet50_x = MaxPooling2D(pool_size=(3, 3))(resnet50_x)\n",
    "resnet50_x = Flatten()(ResNet50_model.output)\n",
    "resnet50_x = Dense(256,activation='relu')(resnet50_x)\n",
    "resnet50_x = Dense(6,activation='softmax')(resnet50_x)\n",
    "resnet50_x_final_model = Model(inputs=ResNet50_model.input, outputs=resnet50_x)\n",
    "resnet50_x_final_model.compile(loss = 'categorical_crossentropy', optimizer= opt, metrics=['acc'])\n",
    "\n",
    "number_of_epochs = resnet_epoch\n",
    "resnet_filepath = 'resnet50'+'-saved-model-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5'\n",
    "resnet_checkpoint = tf.keras.callbacks.ModelCheckpoint(resnet_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "resnet_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, min_lr=0.000002)\n",
    "callbacklist = [resnet_checkpoint,resnet_early_stopping,reduce_lr]\n",
    "resnet50_history = resnet50_x_final_model.fit(train_generator, epochs = number_of_epochs ,validation_data = validation_generator,callbacks=callbacklist,verbose=1)\n",
    "\n",
    "do_history_stuff(resnet50_history, 'resnet50_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could also be the output a different Keras model or layer\n",
    "\n",
    "InceptionV3_model = InceptionV3(input_shape=(150,150,3),weights='imagenet', include_top=False)\n",
    "for layer in InceptionV3_model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in InceptionV3_model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "InceptionV3_last_output = InceptionV3_model.output\n",
    "InceptionV3_maxpooled_output = Flatten()(InceptionV3_last_output)\n",
    "InceptionV3_x = Dense(1024, activation='relu')(InceptionV3_maxpooled_output)\n",
    "InceptionV3_x = Dropout(0.5)(InceptionV3_x)\n",
    "InceptionV3_x = Dense(6, activation='softmax')(InceptionV3_x)\n",
    "InceptionV3_x_final_model = Model(inputs=InceptionV3_model.input,outputs=InceptionV3_x)\n",
    "InceptionV3_x_final_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "number_of_epochs = inception_epoch\n",
    "inception_filepath = 'inceptionv3_'+'-saved-model-{epoch:02d}-loss-{loss:.2f}.hdf5'\n",
    "inception_checkpoint = tf.keras.callbacks.ModelCheckpoint(inception_filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "inception_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "inceptionv3_history = InceptionV3_x_final_model.fit(train_generator, epochs = number_of_epochs, validation_data = validation_generator,callbacks=[inception_checkpoint,inception_early_stopping],verbose=1)\n",
    "\n",
    "do_history_stuff(inceptionv3_history, 'inceptionv3_model', True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_best_model = vgg16_final_model \n",
    "resnet_best_model = resnet50_x_final_model\n",
    "inception_best_model = InceptionV3_x_final_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18377dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mode(my_list):\n",
    "    ct = Counter(my_list)\n",
    "    max_value = max(ct.values())\n",
    "    return ([key for key, value in ct.items() if value == max_value])\n",
    "\n",
    "true_value = []\n",
    "combined_model_pred = []\n",
    "vgg_pred = []\n",
    "resnet_pred = []\n",
    "inception_pred = []\n",
    "benchmark_model_pred = []\n",
    "for folder in os.listdir(seg_test_folders):\n",
    "    \n",
    "    test_image_ids = os.listdir(os.path.join(seg_test_folders,folder))\n",
    "    \n",
    "    for image_id in test_image_ids[:int(len(test_image_ids))]:\n",
    "        \n",
    "        path = os.path.join(seg_test_folders,folder,image_id)\n",
    "        \n",
    "        true_value.append(validation_generator.class_indices[folder])\n",
    "        img = cv2.resize(cv2.imread(path),(150,150))\n",
    "        img_normalized = img/255\n",
    "        #vgg\n",
    "        vgg16_image_prediction = np.argmax(vgg_best_model.predict(np.array([img_normalized])))\n",
    "        vgg_pred.append(vgg16_image_prediction)\n",
    "        \n",
    "        #resnet50\n",
    "        resnet_50_image_prediction = np.argmax(resnet_best_model.predict(np.array([img_normalized])))\n",
    "        resnet_pred.append(resnet_50_image_prediction)\n",
    "        \n",
    "        #Inception\n",
    "        inception_image_prediction = np.argmax(inception_best_model.predict(np.array([img_normalized])))\n",
    "        inception_pred.append(inception_image_prediction)\n",
    "        \n",
    "        #benchmark\n",
    "        benchmark_model_prediction = np.argmax(benchmark_model.predict(np.array([img_normalized])))\n",
    "        benchmark_model_pred.append(benchmark_model_prediction)\n",
    "        \n",
    "        #giving vgg16 high priority if they all predict something different\n",
    "        image_prediction = mode([vgg16_image_prediction, resnet_50_image_prediction, inception_image_prediction])                                  \n",
    "        combined_model_pred.append(image_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddae33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "#from mlxtend.plotting import plot_confusion_matrix\n",
    "def clf_report(true_value, model_pred):\n",
    "    \n",
    "    classes = validation_generator.class_indices.keys()\n",
    "    TP_count = [true_value[i] == model_pred[i] for i in range(len(true_value))]\n",
    "    model_accuracy = np.sum(TP_count)/len(TP_count)\n",
    "    print('Model Accuracy', model_accuracy)\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    cm = confusion_matrix(true_value,model_pred)\n",
    "    plt.imshow(cm,interpolation='nearest',cmap=plt.cm.viridis)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max()*0.8\n",
    "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j,i,cm[i,j],\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"black\" if cm[i,j] > thresh else \"white\")\n",
    "        pass\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    pass\n",
    "    \n",
    "    print(classification_report(true_value, model_pred, target_names = list(classes)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark model\n",
    "clf_report(true_value, benchmark_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8314eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined vote\n",
    "combined_model_pred = [ c[0] for c in combined_model_pred]\n",
    "clf_report(true_value, combined_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG model classification report\n",
    "clf_report(true_value, vgg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474360ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 model classification report\n",
    "clf_report(true_value, resnet_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception model classification report\n",
    "clf_report(true_value, inception_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed138bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(1,benchmark_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a640aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(1,vgg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(1,resnet_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(1,inception_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_images(1,combined_model_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
